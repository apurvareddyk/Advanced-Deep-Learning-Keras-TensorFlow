{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPawrLxce5Di6bzZakg8Ra9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Part 1h: TensorFlow Keras - Callbacks and TensorBoard\n","\n","**Description:**\n","\n","This Colab demonstrates the use of Keras callbacks to monitor and manage the training of a neural network. We will use the `ModelCheckpoint` callback to save the best model weights during training, the `EarlyStopping` callback to prevent overfitting, and the `TensorBoard` callback to visualize training metrics, model graphs, and more in the TensorBoard web interface.\n","\n","We will train a simple neural network on the `digits` dataset and use these callbacks to enhance the training process."],"metadata":{"id":"xXY3zAYGTz5I"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_digits\n","from sklearn.preprocessing import StandardScaler\n","import datetime\n","import os\n","\n","# Load the digits dataset\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","\n","# Scale the data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# Define the model\n","model = models.Sequential([\n","    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n","    layers.Dense(10, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Define the callbacks\n","# 1. ModelCheckpoint: Save the model with the best validation accuracy\n","checkpoint_filepath = 'model_checkpoint/best_model.keras'  # Changed the filepath to end with .keras\n","model_checkpoint_callback = ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_best_only=True,\n","    monitor='val_accuracy',\n","    mode='max',\n","    verbose=1\n",")\n","\n","# 2. EarlyStopping: Stop training when validation loss doesn't improve\n","early_stopping_callback = EarlyStopping(\n","    monitor='val_loss',\n","    patience=10,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","# 3. TensorBoard: Log metrics and more for visualization\n","log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","# Train the model with the callbacks\n","history = model.fit(\n","    X_train, y_train,\n","    epochs=100,\n","    validation_data=(X_test, y_test),\n","    callbacks=[model_checkpoint_callback, early_stopping_callback, tensorboard_callback],\n","    verbose=0  # Reduce verbosity during training\n",")\n","\n","# Load the best model saved by ModelCheckpoint\n","best_model = tf.keras.models.load_model(checkpoint_filepath)\n","\n","# Evaluate the best model on the test set\n","loss, accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n","print(f\"\\nBest Model - Test Accuracy: {accuracy}\")\n","\n","# Instructions on how to view TensorBoard logs in Colab\n","print(\"\\nInstructions to view TensorBoard:\")\n","print(f\"1. Run the following command in a new code cell: `!tensorboard --logdir {log_dir}`\")\n","print(\"2. Click on the link that TensorBoard provides.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hrjW1_idTznH","executionInfo":{"status":"ok","timestamp":1744333647713,"user_tz":420,"elapsed":22073,"user":{"displayName":"Apurva Karne","userId":"15669434470397290511"}},"outputId":"098bb6dd-141b-4741-93e8-eac751a49aae"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: val_accuracy improved from -inf to 0.73333, saving model to model_checkpoint/best_model.keras\n","\n","Epoch 2: val_accuracy improved from 0.73333 to 0.87222, saving model to model_checkpoint/best_model.keras\n","\n","Epoch 3: val_accuracy improved from 0.87222 to 0.91944, saving model to model_checkpoint/best_model.keras\n","\n","Epoch 4: val_accuracy improved from 0.91944 to 0.93056, saving model to model_checkpoint/best_model.keras\n","\n","Epoch 5: val_accuracy improved from 0.93056 to 0.95278, saving model to model_checkpoint/best_model.keras\n","\n","Epoch 6: val_accuracy improved from 0.95278 to 0.95833, saving model to model_checkpoint/best_model.keras\n","\n","Epoch 7: val_accuracy improved from 0.95833 to 0.96111, saving model to model_checkpoint/best_model.keras\n","\n","Epoch 8: val_accuracy did not improve from 0.96111\n","\n","Epoch 9: val_accuracy improved from 0.96111 to 0.96389, saving model to model_checkpoint/best_model.keras\n","\n","Epoch 10: val_accuracy did not improve from 0.96389\n","\n","Epoch 11: val_accuracy did not improve from 0.96389\n","\n","Epoch 12: val_accuracy did not improve from 0.96389\n","\n","Epoch 13: val_accuracy improved from 0.96389 to 0.96944, saving model to model_checkpoint/best_model.keras\n","\n","Epoch 14: val_accuracy improved from 0.96944 to 0.97500, saving model to model_checkpoint/best_model.keras\n","\n","Epoch 15: val_accuracy did not improve from 0.97500\n","\n","Epoch 16: val_accuracy did not improve from 0.97500\n","\n","Epoch 17: val_accuracy did not improve from 0.97500\n","\n","Epoch 18: val_accuracy improved from 0.97500 to 0.97778, saving model to model_checkpoint/best_model.keras\n","\n","Epoch 19: val_accuracy did not improve from 0.97778\n","\n","Epoch 20: val_accuracy did not improve from 0.97778\n","\n","Epoch 21: val_accuracy did not improve from 0.97778\n","\n","Epoch 22: val_accuracy did not improve from 0.97778\n","\n","Epoch 23: val_accuracy did not improve from 0.97778\n","\n","Epoch 24: val_accuracy did not improve from 0.97778\n","\n","Epoch 25: val_accuracy did not improve from 0.97778\n","\n","Epoch 26: val_accuracy did not improve from 0.97778\n","\n","Epoch 27: val_accuracy did not improve from 0.97778\n","\n","Epoch 28: val_accuracy did not improve from 0.97778\n","\n","Epoch 29: val_accuracy did not improve from 0.97778\n","\n","Epoch 30: val_accuracy did not improve from 0.97778\n","\n","Epoch 31: val_accuracy did not improve from 0.97778\n","\n","Epoch 32: val_accuracy did not improve from 0.97778\n","\n","Epoch 33: val_accuracy did not improve from 0.97778\n","\n","Epoch 34: val_accuracy did not improve from 0.97778\n","\n","Epoch 35: val_accuracy did not improve from 0.97778\n","\n","Epoch 36: val_accuracy did not improve from 0.97778\n","\n","Epoch 37: val_accuracy did not improve from 0.97778\n","\n","Epoch 38: val_accuracy did not improve from 0.97778\n","\n","Epoch 39: val_accuracy did not improve from 0.97778\n","\n","Epoch 40: val_accuracy did not improve from 0.97778\n","\n","Epoch 41: val_accuracy did not improve from 0.97778\n","\n","Epoch 42: val_accuracy did not improve from 0.97778\n","\n","Epoch 43: val_accuracy did not improve from 0.97778\n","\n","Epoch 44: val_accuracy did not improve from 0.97778\n","\n","Epoch 45: val_accuracy did not improve from 0.97778\n","\n","Epoch 46: val_accuracy did not improve from 0.97778\n","\n","Epoch 47: val_accuracy did not improve from 0.97778\n","\n","Epoch 48: val_accuracy did not improve from 0.97778\n","\n","Epoch 49: val_accuracy did not improve from 0.97778\n","\n","Epoch 50: val_accuracy did not improve from 0.97778\n","\n","Epoch 51: val_accuracy did not improve from 0.97778\n","\n","Epoch 52: val_accuracy did not improve from 0.97778\n","\n","Epoch 53: val_accuracy did not improve from 0.97778\n","\n","Epoch 54: val_accuracy did not improve from 0.97778\n","Epoch 54: early stopping\n","Restoring model weights from the end of the best epoch: 44.\n","\n","Best Model - Test Accuracy: 0.9777777791023254\n","\n","Instructions to view TensorBoard:\n","1. Run the following command in a new code cell: `!tensorboard --logdir logs/20250411-010705`\n","2. Click on the link that TensorBoard provides.\n"]}]},{"cell_type":"markdown","source":["## Results for Part 1h: TensorFlow Keras - Callbacks and TensorBoard\n","\n","In this experiment, we demonstrated the use of three key Keras callbacks during the training of a simple neural network on the `digits` dataset: `ModelCheckpoint`, `EarlyStopping`, and `TensorBoard`.\n","\n","* **`ModelCheckpoint`:** This callback was configured to save the model weights to the file `model_checkpoint/best_model.keras` whenever the validation accuracy (`val_accuracy`) improved. The output during training shows that the model was saved multiple times as the validation accuracy increased, reaching a peak of 0.97778.\n","\n","* **`EarlyStopping`:** This callback monitored the validation loss (`val_loss`) and was set to stop training if no improvement was observed for 10 consecutive epochs (`patience=10`). The output indicates that training was stopped early after 54 epochs because the validation loss did not improve for 10 epochs. Importantly, the callback also restored the best weights found during training (from epoch 44 in this run).\n","\n","* **`TensorBoard`:** This callback logged various metrics (like loss and accuracy), the model graph, and histograms of weights and biases to the `logs/20250411-010705` directory. These logs can be visualized using the TensorBoard web interface to gain deeper insights into the training process.\n","\n","The test accuracy of the best model (as restored by `EarlyStopping`) was:\n","\n","* **Best Model - Test Accuracy:** 0.9778\n","\n","**Analysis:**\n","\n","The use of callbacks significantly enhanced the training process:\n","\n","* `ModelCheckpoint` ensured that we had a saved version of the model that performed best on the validation set, protecting against potential overfitting in later epochs.\n","* `EarlyStopping` automatically stopped the training when further epochs were unlikely to yield better generalization, saving computational resources and preventing overfitting. The restoration of the best weights ensures that we evaluate the model at its optimal point during training.\n","* `TensorBoard` provides a powerful tool for visualizing and debugging the training process. By launching TensorBoard with the specified log directory, one can observe the trends in loss and accuracy, examine the distribution of weights and biases, and understand the model's architecture.\n","\n","**A/B Test (Conceptual):**\n","\n","An A/B test here could involve comparing the performance of a model trained with these callbacks to one trained for a fixed number of epochs without them. The model with callbacks would likely show better or comparable performance with potentially fewer training epochs and a saved \"best\" version. Another A/B test could compare different `patience` values for `EarlyStopping` to see how it affects the training duration and final model performance.\n","\n","To further explore this, you can now run the command `!tensorboard --logdir logs/20250411-010705` in a new Colab cell and examine the visualizations.\n","\n","Ready to move on to **Part 1i: Using Keras Tuner**?"],"metadata":{"id":"jcCHP4foihfp"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cYlhnqnQTweO","executionInfo":{"status":"ok","timestamp":1744335832057,"user_tz":420,"elapsed":2053850,"user":{"displayName":"Apurva Karne","userId":"15669434470397290511"}},"outputId":"581deb09-cafc-491e-e16e-e5e159d7c06d"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-04-11 01:09:39.373325: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1744333779.418056    7151 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1744333779.429485    7151 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-04-11 01:09:44.708618: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","\n","NOTE: Using experimental fast data loading logic. To disable, pass\n","    \"--load_fast=false\" and report issues on GitHub. More details:\n","    https://github.com/tensorflow/tensorboard/issues/4784\n","\n","Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n","TensorBoard 2.18.0 at http://localhost:6006/ (Press CTRL+C to quit)\n","Exception ignored in atexit callback: <function clean_up at 0x792f95a43ba0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/api.py\", line 2853, in clean_up\n","    clear_caches()\n","  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/api.py\", line 2878, in clear_caches\n","    pjit._cpp_pjit_cache_explicit_attributes.clear()\n","KeyboardInterrupt: \n"]}],"source":["!tensorboard --logdir logs/20250411-010705"]},{"cell_type":"code","source":[],"metadata":{"id":"gRQObMXiZP9F"},"execution_count":null,"outputs":[]}]}