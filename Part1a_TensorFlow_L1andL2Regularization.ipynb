{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyObs8rFlRAgS4qMKQ5OITrP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Part 1a: TensorFlow - L1 and L2 Regularization\n","\n","**Description:**\n","\n","This Colab demonstrates the implementation of L1 and L2 regularization techniques using TensorFlow Keras. We will train simple neural network models on the `digits` dataset with and without these regularizations to observe their impact on model performance and generalization.\n","\n","* **L1 Regularization (Lasso):** Adds a penalty to the loss function proportional to the absolute value of the weights.\n","* **L2 Regularization (Ridge):** Adds a penalty to the loss function proportional to the square of the weights.\n","* **Combined L1 and L2 Regularization:** Using both penalties simultaneously."],"metadata":{"id":"gcW4X4hDJiVq"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RgPs_D_gJISL","executionInfo":{"status":"ok","timestamp":1744330073010,"user_tz":420,"elapsed":74164,"user":{"displayName":"Apurva Karne","userId":"15669434470397290511"}},"outputId":"e408b6e3-3a9b-4cef-aff7-fbc4d373e06d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluation:\n","No Regularization - Test Accuracy: 0.9750000238418579\n","L2 Regularization - Test Accuracy: 0.9750000238418579\n","L1 Regularization - Test Accuracy: 0.9583333134651184\n","L1 and L2 Regularization - Test Accuracy: 0.9611111283302307\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, models, regularizers\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_digits\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the digits dataset\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","\n","# Scale the data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# Define a function to create a simple model with regularization\n","def create_regularized_model(l1=0.0, l2=0.0):\n","    return models.Sequential([\n","        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), input_shape=(X_train.shape[1],)),\n","        layers.Dense(10, activation='softmax')\n","    ])\n","\n","# Model without regularization (baseline)\n","model_no_reg = models.Sequential([\n","    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n","    layers.Dense(10, activation='softmax')\n","])\n","model_no_reg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","history_no_reg = model_no_reg.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=0)\n","\n","# Model with L2 regularization\n","model_l2 = create_regularized_model(l2=0.01)\n","model_l2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","history_l2 = model_l2.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=0)\n","\n","# Model with L1 regularization\n","model_l1 = create_regularized_model(l1=0.01)\n","model_l1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","history_l1 = model_l1.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=0)\n","\n","# Model with both L1 and L2 regularization\n","model_l1_l2 = create_regularized_model(l1=0.01, l2=0.01)\n","model_l1_l2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","history_l1_l2 = model_l1_l2.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=0)\n","\n","# Evaluate the models\n","print(\"\\nEvaluation:\")\n","print(\"No Regularization - Test Accuracy:\", model_no_reg.evaluate(X_test, y_test, verbose=0)[1])\n","print(\"L2 Regularization - Test Accuracy:\", model_l2.evaluate(X_test, y_test, verbose=0)[1])\n","print(\"L1 Regularization - Test Accuracy:\", model_l1.evaluate(X_test, y_test, verbose=0)[1])\n","print(\"L1 and L2 Regularization - Test Accuracy:\", model_l1_l2.evaluate(X_test, y_test, verbose=0)[1])"]},{"cell_type":"markdown","source":["## Results for Part 1a: TensorFlow - L1 and L2 Regularization\n","\n","In this experiment, we trained four simple neural network models on the `digits` dataset to evaluate the impact of L1 and L2 regularization:\n","\n","* **Model without Regularization (Baseline):** This model served as our control.\n","* **Model with L2 Regularization (l2=0.01):** This model applied L2 regularization to the weights of the dense layers.\n","* **Model with L1 Regularization (l1=0.01):** This model applied L1 regularization to the weights of the dense layers.\n","* **Model with Combined L1 and L2 Regularization (l1=0.01, l2=0.01):** This model applied both L1 and L2 regularization to the weights.\n","\n","The test accuracies achieved by each model are as follows:\n","\n","* **No Regularization - Test Accuracy:** 0.9750\n","* **L2 Regularization - Test Accuracy:** 0.9750\n","* **L1 Regularization - Test Accuracy:** 0.9583\n","* **L1 and L2 Regularization - Test Accuracy:** 0.9611\n","\n","**Analysis:**\n","\n","The results indicate that in this particular experiment on the `digits` dataset with this simple network architecture and these regularization strengths:\n","\n","* **L2 regularization** did not show a noticeable improvement or degradation in test accuracy compared to the baseline model without regularization.\n","* **L1 regularization** resulted in a slightly lower test accuracy compared to the baseline. L1 regularization encourages sparsity in the weights, which might have led to some loss of information in this relatively small and well-structured dataset.\n","* The **combination of L1 and L2 regularization** also resulted in a slightly lower test accuracy than the baseline, although it performed better than L1 regularization alone.\n","\n","It's important to note that the optimal regularization strength ($\\lambda$ in L1 and L2, controlled by `l1` and `l2` parameters) is highly dependent on the specific dataset and model architecture. Further experimentation with different regularization strengths might yield different results.\n"],"metadata":{"id":"lU1K0zvVMMHw"}},{"cell_type":"code","source":[],"metadata":{"id":"D0phrQ8QK4GK"},"execution_count":null,"outputs":[]}]}