{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZF4vdlaumIt09rrE7qP7z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Part 1g: TensorFlow - Custom Dropout and Custom Regularization\n","\n","**Description:**\n","\n","This Colab demonstrates how to implement custom dropout behavior and custom regularization techniques in TensorFlow Keras. While Keras provides built-in layers for standard dropout and regularizers like L1 and L2, you might have specific requirements that necessitate custom implementations.\n","\n","**Custom Dropout:** We will create a custom layer that implements a specific dropout pattern (e.g., dropping out entire feature maps instead of individual neurons, although for simplicity we'll stick to a similar neuron-wise dropout but as a custom layer).\n","\n","**Custom Regularization:** We will define a custom regularizer function that applies a specific penalty to the weights of a layer, different from the standard L1 or L2. For this example, we will implement a custom L1 regularizer with a potentially different scaling mechanism."],"metadata":{"id":"nRLq-dbOQkhE"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MjSyrXMKQgl4","executionInfo":{"status":"ok","timestamp":1744332155037,"user_tz":420,"elapsed":34955,"user":{"displayName":"Apurva Karne","userId":"15669434470397290511"}},"outputId":"5a232799-0396-4364-cff8-abba8f1aea29"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Model with Custom Dropout and Regularization - Test Accuracy: 0.9305555820465088\n","Model with Standard Dropout and L1 Regularization - Test Accuracy: 0.980555534362793\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, models, regularizers\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras import backend as K\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_digits\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the digits dataset\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","\n","# Scale the data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# 1. Custom Dropout Layer - Corrected to use TensorFlow random ops\n","class CustomDropout(Layer):\n","    def __init__(self, rate, **kwargs):\n","        super(CustomDropout, self).__init__(**kwargs)\n","        self.rate = min(1., max(0., rate))\n","        self.supports_masking = True\n","\n","    def call(self, inputs, training=None):\n","        if training is None or not training:\n","            return inputs\n","        keep_prob = 1. - self.rate\n","        random_tensor = tf.random.uniform(shape=tf.shape(inputs))\n","        mask = tf.cast(random_tensor >= self.rate, dtype=inputs.dtype)\n","        return inputs * mask / keep_prob\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({'rate': self.rate})\n","        return config\n","\n","# 2. Custom Regularizer (Modified L1) - No changes needed here\n","class CustomL1Regularizer(regularizers.Regularizer):\n","    def __init__(self, factor):\n","        self.factor = factor\n","\n","    def __call__(self, weight_matrix):\n","        return self.factor * K.sum(K.abs(weight_matrix)) ** 2  # Example: L1 penalty squared\n","\n","    def get_config(self):\n","        return {'factor': self.factor}\n","\n","# Create models using custom dropout and regularization\n","model_custom = models.Sequential([\n","    layers.Dense(64, activation='relu', kernel_regularizer=CustomL1Regularizer(0.001), input_shape=(X_train.shape[1],)),\n","    CustomDropout(0.5),\n","    layers.Dense(10, activation='softmax')\n","])\n","\n","model_custom.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","history_custom = model_custom.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=0)\n","evaluation_custom = model_custom.evaluate(X_test, y_test, verbose=0)\n","loss_custom = evaluation_custom[0]\n","accuracy_custom = evaluation_custom[1]\n","print(f\"Model with Custom Dropout and Regularization - Test Accuracy: {accuracy_custom}\")\n","\n","# Compare with a model using standard Dropout and L1 regularization\n","model_standard = models.Sequential([\n","    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.001), input_shape=(X_train.shape[1],)),\n","    layers.Dropout(0.5),\n","    layers.Dense(10, activation='softmax')\n","])\n","\n","model_standard.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","history_standard = model_standard.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=0)\n","evaluation_standard = model_standard.evaluate(X_test, y_test, verbose=0)\n","loss_standard = evaluation_standard[0]\n","accuracy_standard = evaluation_standard[1]\n","print(f\"Model with Standard Dropout and L1 Regularization - Test Accuracy: {accuracy_standard}\")"]},{"cell_type":"markdown","source":["## Results for Part 1g: TensorFlow - Custom Dropout and Custom Regularization\n","\n","In this experiment, we implemented a custom dropout layer (`CustomDropout`) and a custom L1 regularizer (`CustomL1Regularizer`) in TensorFlow Keras. We then trained a simple neural network using these custom components and compared its performance to a network using standard Keras `Dropout` and `L1` regularization. Both models were trained on the `digits` dataset for 50 epochs.\n","\n","The test accuracies achieved by each model are as follows:\n","\n","* **Model with Custom Dropout and Regularization - Test Accuracy:** 0.9306\n","* **Model with Standard Dropout and L1 Regularization - Test Accuracy:** 0.9806\n","\n","**Analysis:**\n","\n","The results indicate a noticeable difference in performance between the two models:\n","\n","* The model utilizing the **standard Keras Dropout and L1 regularization** achieved a significantly higher test accuracy of 0.9806 compared to the 0.9306 achieved by the model with the custom implementations.\n","\n","This suggests that, for this specific task and the chosen hyperparameters:\n","\n","* The standard Keras `Dropout` layer and `L1` regularizer were more effective in promoting generalization.\n","* Our custom `CustomDropout` implementation, while functionally similar to standard dropout, might have subtle differences in its interaction with the rest of the network or the optimization process that led to lower performance.\n","* The custom `CustomL1Regularizer`, which applied a squared L1 norm penalty, also appears to be less effective than the standard L1 regularizer (which applies the absolute value penalty) in this scenario.\n","\n","**A/B Test Comparison:**\n","\n","This experiment highlights the importance of carefully implementing and testing custom layers and regularizers. While Keras provides flexibility for customization, the built-in components are often well-optimized and have been proven effective across a wide range of tasks. In this case, the standard Keras regularization and dropout outperformed our custom versions. Further investigation and potential refinement of the custom implementations would be needed to understand why they underperformed."],"metadata":{"id":"_OQbTiAeTaKZ"}},{"cell_type":"code","source":[],"metadata":{"id":"qvpre2wCQnrW"},"execution_count":null,"outputs":[]}]}